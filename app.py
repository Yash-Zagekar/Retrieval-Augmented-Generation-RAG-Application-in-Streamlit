'''
Name - Yash Zagekar
Contact - yashzagekar820@gmail.com
Task 4: Create a Retrieval Augmented Generation (RAG) Application in Streamlit
Complexity: Medium
User-Story: Build an interactive Streamlit application where users can upload multiple documents and chat
with those documents using RAG.
Hints:
‚óè Implement a file uploader to allow users to upload multiple documents (PDF, DOCX, TXT).
‚óè Parse and preprocess the uploaded documents.
‚óè Use a Large language model and a retrieval mechanism to augment the generation.
‚óè Integrate with a document retrieval library to fetch relevant passages from the uploaded documents.
‚óè Build a chat interface where users can ask questions.
‚óè Display responses generated by the RAG model using the retrieved document passages.
‚óè Test the application with various documents and queries.
‚óè Submit the screen recording of the working application.
'''

import streamlit as st
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings
import google.generativeai as genai
from langchain.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains.question_answering import load_qa_chain
from langchain.prompts import PromptTemplate
import os

api_key = 'AIzaSyC2dXe_sPE3QfEdkFQrlp3s-S2dduAGRTA'

if not api_key:
    st.error("API Key is missing. Please set the GOOGLE_API_KEY environment variable.")
    st.stop()
st.set_page_config(page_title="Chat with documents", layout="wide")

st.markdown(
    """
    <style>
    @import url('https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;500;600&display=swap');

    
    html, body, div, span, a, button, input, select, textarea, p, h1, h2, h3, h4, h5, h6, label, table, th, td {
        font-family: 'Poppins', sans-serif !important;
    }

    /* Force the font for Streamlit-specific components */
    .css-1v3fvcr, .css-18e3th9, .css-1h6h10h, .css-1xtdkn7, .stText, .stMarkdown, .stButton, .stFileUploader, .stRadio, .stSlider {
        font-family: 'Poppins', sans-serif !important;
    }

    </style>
    """,
    unsafe_allow_html=True
)
st.markdown("""
## Chat with documents : Receive Immediate Insights from documents

This chatbot is built using the Retrieval-Augmented Generation (RAG) framework, leveraging Google's Generative AI model Gemini-PRO.

### üåü How It Works

Follow these simple steps to interact with the chatbot:

1. **Upload Your Documents**: Upload documents(txt,pdf,docx). click submit and process

2. **Ask a Question**: After processing the documents, ask any question related to the content of your uploaded documents for a precise answer.
""")

# üí° Function to a extract content/text from documents - supported (docx,txt,pdf)
from PyPDF2 import PdfReader
import docx
def get_pdf_text(pdf_docs):
    text = ""
    for pdf in pdf_docs:
        if pdf.type == 'application/pdf':
            pdf_reader = PdfReader(pdf)
            for page in pdf_reader.pages:
                text += page.extract_text()
        elif pdf.type == 'text/plain':
            text += pdf.read().decode("utf-8")
        elif pdf.type == 'application/vnd.openxmlformats-officedocument.wordprocessingml.document':
            doc = docx.Document(pdf)
            for para in doc.paragraphs:
                text += para.text + "\n"
    return text



# üí° Text chunking - To manage large content by dividing to into small chunks
def get_text_chunks(text):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)
    chunks = text_splitter.split_text(text)
    return chunks


# Vector database - for storing embedded language (Model understandable language)
def get_vector_store(text_chunks, api_key):
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=api_key)
    vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)
    vector_store.save_local("faiss_index")

# Template prompting - to customize output
def get_conversational_chain():
    prompt_template = """
    Answer the question precisely from the given content, make sure you include everything mentioned in details, in case if the answer is not in
    provided context just say, "answer is not available in the document", don't provide the wrong, irrelevant answer\n\n
    Context:\n {context}?\n
    Question: \n{question}\n

    Answer:
    """
    model = ChatGoogleGenerativeAI(model="gemini-pro", temperature=0.3, google_api_key=api_key)
    prompt = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
    chain = load_qa_chain(model, chain_type="stuff", prompt=prompt)
    return chain

# user input handling
def user_input(user_question, api_key):
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=api_key)
    new_db = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
    docs = new_db.similarity_search(user_question)
    chain = get_conversational_chain()
    response = chain({"input_documents": docs, "question": user_question}, return_only_outputs=True)
    st.write("Model reply: ", response["output_text"])

def main():
    st.header("ü§ñ Model")

    user_question = st.text_input("Ask a Question from the documents", key="user_question")

    if user_question:
        user_input(user_question, api_key)

    with st.sidebar:
        st.title("Menu:")
        pdf_docs = st.file_uploader("Upload your documents and Click on the Submit & Process Button", accept_multiple_files=True, key="pdf_uploader")
        if st.button("Submit & Process"):
            with st.spinner("Processing..."):
                raw_text = get_pdf_text(pdf_docs)
                text_chunks = get_text_chunks(raw_text)
                get_vector_store(text_chunks, api_key)
                st.success("Done")

if __name__ == "__main__":
    main()
